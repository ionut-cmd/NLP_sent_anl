# EX 1
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.KLDivLoss(reduction='batchmean)

prediction= sadness

auc = {0: 0.5698394425138708, 1: 0.45561698319578403, 2: 0.5162782934637473, 3: 0.5619247358660555, 4: 0.6793870253703389, 5: 0.5246719160104987, 6: 0.5824794839260965, 7: 0.598467398865367, 8: 0.599034288952545, 9: 0.6199381598701439, 10: 0.5648409210757602, 11: 0.702361382619513, 12: 0.6010919434758671, 13: 0.5995726026561762}

cm = [[   0    0    0    1   11    0    0    0    0    0    0    0    0  599]
 [   0    0    0    1    0    0    0    0    0    0    0    0    0   88]
 [   0    0    0    0    1    0    0    0    0    0    0    0    0   87]
 [   0    0    0    3   27    0    0    0    0    0    0    0    0  836]
 [   0    0    0    1  143    0    0    0    0    0    0    0    0  695]
 [   0    0    0    1    1    0    0    0    0    0    0    0    0   22]
 [   0    0    0    6    2    0    0    0    0    0    0    0    0  430]
 [   0    0    0    2   11    0    0    0    0    0    0    0    0  333]
 [   0    0    0    2   12    0    0    0    0    0    0    0    0  220]
 [   0    0    0    3    1    0    0    0    0    0    0    0    0  173]
 [   0    0    0    0    2    0    0    0    0    0    0    0    0   48]
 [   0    0    0    1    3    0    0    0    0    0    0    0    0  108]
 [   0    0    0    0    6    0    0    0    0    0    0    0    0  284]
 [   0    0    0    2   20    0    0    0    0    0    0    0    0 1553]]





