# EX 1
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.KLDivLoss(reduction='batchmean)

cm = [[212   5   9  49  33   0  34  11   2  20   1   4  15 216]
 [ 31  12   5   4   2   0   5   1   1   2   0   0   1  25]
 [  8   2  23   6   6   0  10   6   1   0   0   1   1  24]
 [ 48   0   0 353 146   0  28  22  17  14   0   9  12 217]
 [ 21   2   3  70 561   0  27  16  16   8   0   2  13 100]
 [  3   0   0   7   5   3   1   1   0   0   0   0   1   3]
 [ 50   6  12  26  28   0 161  12   7   3   0   3   8 122]
 [ 29   1   7  43  36   0  25  61   3   4   0   1  10 126]
 [ 10   0   1  26  45   0   4   2  77   2   0   7   1  59]
 [ 12   0   1   8   5   0   8  10   0  47   0   1  27  58]
 [ 10   3   0   4   5   0  13   2   1   0   3   2   0   7]
 [  5   0   0   8  22   0   2   2  11   0   0  28   4  30]
 [ 13   0   2  26  15   0   9   8   3  21   0   1  86 106]
 [171   3  14 180 124   0  46  25  24  29   1  18  51 889]]

 auc = {0: 0.767304628355347, 1: 0.8159172715521527, 2: 0.8266477373272632, 3: 0.7719668494305, 4: 0.8737410911921384, 5: 0.684222805482648, 6: 0.8037640869001197, 7: 0.718289283153392, 8: 0.8126543856789088, 9: 0.8544522252492194, 10: 0.7320056248901389, 11: 0.8435857726776512, 12: 0.8879408433056365, 13: 0.7241896528063675}


